{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep-learning \\#1#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization\n",
    "\n",
    "In this class we briefly introduce the fundamentals of modern Deep Learning techniques by means of the [**Keras Library**](https://keras.io/).\n",
    "\n",
    "Deep Learning methods require **many** matrix multiplications, which can be greatly accelerated using GPUs. There are four major Deep Learning libraries out there, that are optimized to implement these techniques while leveraging GPU computing capabilities:\n",
    "\n",
    "* **TensorFlow**: https://www.tensorflow.org/\n",
    "* **Theano**: http://deeplearning.net/software/theano/\n",
    "* **Caffe**: http://caffe.berkeleyvision.org/\n",
    "* **Torch**: http://torch.ch/\n",
    "\n",
    "**Keras** is a thin wrapper that works on top of either Theano or TensorFlow. As TF does not work easily on Windows systems, we will be using Theano as the backend for Keras in these two sessions.\n",
    "\n",
    "Keras was developed and is maintained by François Chollet from Google Inc. It is completely written in Python, leaving to Theano/TF the GPU computing part (that is written in CUDA - a C++ extension for NVIDA grpahic cards). With Keras, we do not need to know how to program the GPU to develop and optimize new Deep Learning models. The focus is on rapid **prototyping**, easy **extendibility**, **modularity** and **minimalism**.\n",
    "\n",
    "Let us quickly verify that we are running the Theano backend. As these computer do not have GPUs, we will be running our examples on the CPU (**much slower**). You can gain cheap access to GPU computing capabilities on the Amazon Web Services (AWS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano, keras\n",
    "print theano.config.device \n",
    "print keras.backend._BACKEND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard neural network is composed of a bunch of **neurons**, **weights** and **activation functions**. They are all combined to build a complex non-linear function that can be fit to our **input** data by an efficient training method called **backpropagation**.\n",
    "\n",
    "<img src=\"im_1.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementing a Neural Network classifier with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the MNIST dataset, a standard toy dataset for image classification. flatten the images, convert the class labels, and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Some visual inspection\n",
    "plt.imshow(X_train[0].reshape(28,28), cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.title('This number label: ' + str(y_train[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed a neural net, we need to flatten the images, convert the class labels, and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "X_train = X_train.reshape(60000, 28**2).astype('float32') / 255\n",
    "X_test = X_test.reshape(10000, 28**2).astype('float32') / 255\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, the core data structure is called a **model**, and it dictates how layers in your Deep Neural Network are organized. The most common model is a `Sequential`, standing for a sequential (linear) stack of layers. \n",
    "\n",
    "To give content to a model, you create a **`Sequential`**, and add layers to it that determine the computations that will be done. When finished, you compile your model (including specifying which loss function and optimizer you want to employ), and voilà: you have a sk-learn-style object, that can be fit to your data with a one-liner **`model.fit()`**. Once the model is fit to your data, you can make predictions on new unseen data, using **`model.predict()`**, and assess the quality of your model with **`model.evaluate()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(28 * 28,) , init = 'uniform', activation = 'sigmoid'))\n",
    "model.add(Dense(10, init = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=64, nb_epoch=10,\n",
    "          verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Test classification rate %0.05f \" % model.evaluate(X_test, Y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some result visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['validation accuracy'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict classes on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_hat = model.predict_classes(X_test)\n",
    "pd.crosstab(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_wrong = [im for im in zip(X_test,y_hat,y_test) if im[1] != im[2]]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for ind, val in enumerate(test_wrong[:20]):\n",
    "    plt.subplot(10, 10, ind + 1)\n",
    "    im = 1 - val[0].reshape((28,28))\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(0, 0, val[2], fontsize=14, color='green') # correct \n",
    "    plt.text(8, 0, val[1], fontsize=14, color='red')  # predicted\n",
    "    plt.imshow(im, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras and sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNNs come with a number of hyperparameters: \n",
    "* Which optimizer do I use? Learning rate? Batch size?\n",
    "* How many neurons in each layer?\n",
    "* How many layers?\n",
    "* Which kind of regularization? And their associated parameters\n",
    "* Which initialization\n",
    "\n",
    "Most of your time will probably be invested in tweaking these :(\n",
    "\n",
    "Fortunately, we can import some functionalities of sklearn to tune hyperparameters. To that end, we will be employing the `KerasClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This that takes as argument a function that creates and compiles a model with a fixed hyperparameter configuration. We thus need to define that model-generating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_new_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(28 * 28,) , init = 'uniform', activation = 'sigmoid'))\n",
    "    model.add(Dense(10, init = 'uniform', activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can feed the `create_new_model` function to `KerasClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = create_new_model, nb_epoch = 10, batch_size=32, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the parameters `nb_epoch` and `batch_size`, that are passed. They will be passed to the `fit()` function inside the `KerasClassifier` class.\n",
    "\n",
    "With this, we may for instance evaluate a model using k-fold cross-validation from `scikit learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(10000, 28**2).astype('float32') / 255\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(y_train, n_folds = 2, shuffle = True, random_state=1)\n",
    "\n",
    "results = cross_val_score(model, X_train, Y_train, cv = kfold)\n",
    "\n",
    "print '\\n', results.mean(), ' +/- ', results.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform hyperparameter tuning with grid search from `scikit learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_new_model_2(init = 'uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(28 * 28,) , init = init, activation = 'sigmoid'))\n",
    "    model.add(Dense(10, init = init, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can feed the `create_new_model_2` function to `KerasClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = create_new_model_2, nb_epoch = 8, batch_size=32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "init = ['uniform', 'normal']\n",
    "#batches = np.array([32, 48])\n",
    "\n",
    "# hyper_parameters_grid = dict(optimizer=optimizers, nb_epochs = epochs, batch_size = batches, init = init)\n",
    "\n",
    "hyper_parameters_grid = dict(init = init) #Turn off cv for speed\n",
    "\n",
    "grid = GridSearchCV(estimator = model, param_grid = hyper_parameters_grid, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for params, mean_score, scores in grid_result.grid_scores_:\n",
    "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Other things: Learning rate, regularization: Dropout, L², etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to skip this, but we comment on it a little bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convolutional Neural Networks in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall this step in dataset loading above: \n",
    "\n",
    "`X_train = X_train.reshape(60000, 28**2).astype('float32') / 255`\n",
    "\n",
    "Things to reflect about:\n",
    "* This loses all the two-dimensional image structure!\n",
    "* Images are not usually $32\\times32$.\n",
    "* Many parameters connecting far away areas in an image. \n",
    "\n",
    "The solution to this is to build **Convolutional Neural Networks**, in which each group of neurons only look to a localized region on the image, and are not connected to the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main kind of layers that compose a typical Convolutional Neural Network:\n",
    "1. Convolutional layers\n",
    "2. Pooling Layers\n",
    "3. Fully-connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "<img src=\"im_2.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "<img src=\"im_3.png\", width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "<img src=\"im_1.png\", width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, the same as above, they are stacked in a sequential manner:\n",
    "\n",
    "<img src=\"im_4.png\", width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These and many more are implemented in Keras and ready to use. Read the docs:\n",
    "\n",
    " [**Keras Layers**](https://keras.io/layers/core/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasifying MNIST with a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really interesting stuff, setting things up again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Reduce dataset size for convenience here\n",
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "X_test = X_test[:2000]\n",
    "y_test = y_test[:2000]\n",
    "       \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define a convolutional net model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 4\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep CNNs have tons of weights to be tuned; as a consequence these models are really data-hungry. \n",
    "\n",
    "In order to avoid overfitting your data, a popular strategy consists of ''generating'' new data from what you have. This is really easy in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the `ImageDataGenerator` class that defines the configuration for image data preparation and augmentation. This includes capabilities such as:\n",
    "* Sample-wise standardization.\n",
    "* Feature-wise standardization.\n",
    "* ZCA whitening.\n",
    "* Random rotation, shifts, shear and flips.\n",
    "* Dimension reordering.\n",
    "* Save augmented images to disk.\n",
    "\n",
    "An augmented image generator can be created as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`from keras.preprocessing.image import ImageDataGenerator`\n",
    "\n",
    "`datagen = ImageDataGenerator()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than performing the operations on your entire image dataset in memory, the API is designed to be iterated by the deep learning model fitting process, creating augmented image data on the fly. This reduces memory overhead, but adds some additional time cost during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have created and configured your ImageDataGenerator, you must fit it on your data. This will calculate any statistics required to actually perform the transforms to your image data. You can do this by calling the fit() function on the data generator and pass it to your training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`datagen.fit(train)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data generator itself is in fact an iterator, returning batches of image samples when requested. We can configure the batch size and prepare the data generator and get batches of images by calling the flow() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`X_batch, y_batch = datagen.flow(train, train, batch_size=32)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can make use of the data generator. Instead of calling the fit() function on our model, we must call the fit_generator() function and pass in the data generator and the desired length of an epoch as well as the total number of epochs on which to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`fit_generator(datagen, samples_per_epoch=len(train), nb_epoch=10)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**More info**](https://keras.io/preprocessing/image/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Rotations\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(rotation_range=45)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,5,figsize=(15, 15))\n",
    "im = np.zeros((28,28))\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=10):\n",
    "    for ind in range(5):\n",
    "        axarr[ind].imshow(X_batch[ind].reshape(28, 28), cmap=pyplot.get_cmap('gray'))    \n",
    "        axarr[ind].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice example of taking advantage of dataset augmentation to train a dog-cat classifier with few examples can be found [**here**](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
